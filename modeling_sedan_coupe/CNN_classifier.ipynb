{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as p\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#tf.enable_eager_execution()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv('/home/ec2-user/training_data_meta.csv')\n",
    "\n",
    "#Join cleaned class data with training metadata\n",
    "\n",
    "class_data = pd.read_csv('/home/ec2-user/stanford_labels_cleaned.csv')\n",
    "\n",
    "meta_data = pd.merge(meta_data, class_data, on = 'class', how = 'left')\n",
    "\n",
    "meta_data = meta_data.loc[meta_data['Body Type'].isin(['Coupe', 'Sedan'])].copy()\n",
    "\n",
    "meta_data['is_sedan_target'] = (meta_data['Body Type'] == 'Sedan').astype(int)\n",
    "\n",
    "image_dict = p.load(open('/home/ec2-user/scaled_bounded_grayscale_dict.p', 'rb'))\n",
    "\n",
    "training_data = []\n",
    "for i in meta_data[['is_sedan_target', 'fname']].iterrows():\n",
    "    row = [i[1]['is_sedan_target']]\n",
    "    row.extend(image_dict[i[1]['fname']].flatten())\n",
    "    training_data.append(row)\n",
    "\n",
    "training_data = np.array(training_data).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training_data[:,1:], \n",
    "                                                    training_data[:,0], \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=420)\n",
    "\n",
    "y_train = y_train.astype('int32')\n",
    "y_test = y_test.astype('int32')\n",
    "\n",
    "negatives = []\n",
    "for ind, elem in enumerate(y_train):\n",
    "    if elem == 0:\n",
    "        negatives.append(ind)\n",
    "np.random.shuffle(negatives)\n",
    "\n",
    "neg_dupe_target = 2 * y_train.sum() - y_train.shape[0]\n",
    "\n",
    "X_train = np.concatenate([X_train, X_train[negatives[0:neg_dupe_target],:]],\n",
    "                         axis = 0\n",
    "                        )\n",
    "y_train = np.concatenate([y_train, y_train[negatives[0:neg_dupe_target]]],\n",
    "                         axis = 0\n",
    "                        )\n",
    "\n",
    "y_train = np.concatenate([1 - y_train.reshape(-1,1), y_train.reshape(-1,1)], axis = 1)\n",
    "y_test = np.concatenate([1 - y_test.reshape(-1,1), y_test.reshape(-1,1)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    if type(features) is dict:\n",
    "        features = features['input']\n",
    "    #Define model architecture\n",
    "    \n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    input_layer = tf.reshape(features, [-1, 200, 200, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 3x3 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 200, 200, 1]\n",
    "    # Output Tensor Shape: [batch_size, 200, 200, 32]\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 200, 200, 32]\n",
    "    # Output Tensor Shape: [batch_size, 100, 100, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 3x3 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 100, 100, 32]\n",
    "    # Output Tensor Shape: [batch_size, 100, 100, 64]\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 100, 100, 64]\n",
    "    # Output Tensor Shape: [batch_size, 50, 50, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(\n",
    "      inputs=pool2,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #3\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 50, 50, 64]\n",
    "    # Output Tensor Shape: [batch_size, 25, 25, 64]\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 25, 25, 64]\n",
    "    # Output Tensor Shape: [batch_size, 25 * 25 * 64]\n",
    "    pool3_flat = tf.reshape(pool3, [-1, 25 * 25 * 64])\n",
    "\n",
    "    # Dense Layer\n",
    "    # Densely connected layer with 256 neurons\n",
    "    # Input Tensor Shape: [batch_size, 25 * 25 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 256]\n",
    "    dense = tf.layers.dense(inputs=pool3_flat, units=64, activation=tf.nn.relu)\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.75, training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=2)\n",
    "    \n",
    "    #Predict Op\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            # Generate predictions (for PREDICT and EVAL mode)\n",
    "            \"classes\": tf.argmax(input=logits, axis=1),\n",
    "            # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "            # `logging_hook`.\n",
    "            \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\"),\n",
    "            \"logits\" : logits\n",
    "            \n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        #Define model outputs\n",
    "        # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels[:,1], logits=logits)\n",
    "        \n",
    "        logging_hook = tf.train.LoggingTensorHook(\n",
    "            {\"loss\" : loss,\n",
    "            },\n",
    "            every_n_iter=10)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, loss=loss, train_op=train_op, training_hooks = [logging_hook])\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        #Define model outputs\n",
    "        # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels[:,1], logits=logits)\n",
    "        \n",
    "        predictions = {\n",
    "          # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "          # `logging_hook`.\n",
    "          \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "        }\n",
    "        #swap this for AUROC?\n",
    "        eval_metric_ops = {\n",
    "          \"eval_accuracy\": tf.metrics.auc(\n",
    "              labels=labels, predictions=predictions[\"probabilities\"])}\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "          mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_config = tf.ConfigProto(\n",
    "      inter_op_parallelism_threads=32,\n",
    "      intra_op_parallelism_threads=32,\n",
    "      )\n",
    "\n",
    "config = tf.estimator.RunConfig(session_config = session_config,\n",
    "                                log_step_count_steps = 10,\n",
    "                                save_summary_steps = 10\n",
    "                               )\n",
    "\n",
    "# Create the Estimator\n",
    "car_classifier = tf.estimator.Estimator(\n",
    "  model_fn=cnn_model_fn, model_dir=\"/home/ec2-user/convnet_model\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom input function using training_image_processor with tf.data.Dataset\n",
    "def image_processor_train_input_fn(image_processor, X_train, y_train, batch_size):\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator = lambda : image_processor.flow(X_train.reshape((X_train.shape[0], 200, 200, 1)), \n",
    "                                                  y_train, batch_size, shuffle = True), \n",
    "        output_types = (np.float32, np.int32))\n",
    "    \n",
    "    dataset = dataset.shuffle(512).repeat()\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def image_processor_eval_input_fn(X_test, y_test, batch_size):\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_test.astype('float32') / 255.0, \n",
    "                                                  y_test.astype('int32')))\n",
    "        \n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "#Shearing, no zooming\n",
    "training_image_processor = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale= 1./255,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    shear_range=15,\n",
    "    horizontal_flip=True,\n",
    "    data_format = 'channels_last',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "car_classifier.train(\n",
    "      input_fn=lambda:image_processor_train_input_fn(training_image_processor, X_train, y_train, 128),\n",
    "      steps=5000\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From <ipython-input-4-b656b12c85d9>:21: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-b656b12c85d9>:27: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-b656b12c85d9>:69: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-b656b12c85d9>:73: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:788: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-21T18:10:22Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/convnet_model/model.ckpt-5653\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-21-18:11:11\n",
      "INFO:tensorflow:Saving dict for global step 5653: eval_accuracy = 0.9819666, global_step = 5653, loss = 0.17744076\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5653: /home/ec2-user/convnet_model/model.ckpt-5653\n",
      "{'eval_accuracy': 0.9819666, 'loss': 0.17744076, 'global_step': 5653}\n",
      "testing error\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-21T18:11:17Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/convnet_model/model.ckpt-5653\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-21-18:11:35\n",
      "INFO:tensorflow:Saving dict for global step 5653: eval_accuracy = 0.8821624, global_step = 5653, loss = 0.5271366\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5653: /home/ec2-user/convnet_model/model.ckpt-5653\n",
      "{'eval_accuracy': 0.8821624, 'loss': 0.5271366, 'global_step': 5653}\n"
     ]
    }
   ],
   "source": [
    "print('training error')\n",
    "eval_results = car_classifier.evaluate(\n",
    "    input_fn=lambda:image_processor_eval_input_fn(X_train, y_train, 32))\n",
    "print(eval_results)\n",
    "\n",
    "print('testing error')\n",
    "eval_results = car_classifier.evaluate(\n",
    "    input_fn=lambda:image_processor_eval_input_fn(X_test, y_test, 32))\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/convnet_model/model.ckpt-5653\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predicted_classes = car_classifier.predict(\n",
    "    input_fn=lambda:image_processor_eval_input_fn(X_test, y_test, 32))\n",
    "\n",
    "testing_analysis = []\n",
    "\n",
    "for pred, label in zip(predicted_classes, y_test):\n",
    "    hold = [pred['probabilities'][1], pred['classes'], label[1], pred['logits'][0], pred['logits'][1]]\n",
    "    testing_analysis.append(hold)\n",
    "    \n",
    "testing_analysis = pd.DataFrame(testing_analysis, columns = ['prob', 'class', 'label', 'logit_0', 'logit_1'])\n",
    "testing_analysis['logit_1_decile'] = pd.qcut(testing_analysis.logit_1, 10, labels = False)\n",
    "testing_analysis['logit_0_decile'] = pd.qcut(testing_analysis.logit_0, 10, labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logit_1_decile  label  class\n",
       "0               0      0        103\n",
       "                1      0          6\n",
       "1               0      0         90\n",
       "                1      0         18\n",
       "2               0      0         82\n",
       "                1      0         27\n",
       "3               0      0         66\n",
       "                1      0         42\n",
       "4               0      0         28\n",
       "                       1         27\n",
       "                1      1         37\n",
       "                       0         17\n",
       "5               0      1         21\n",
       "                1      1         87\n",
       "6               0      1         13\n",
       "                1      1         95\n",
       "7               0      1         13\n",
       "                1      1         96\n",
       "8               0      1          8\n",
       "                1      1        100\n",
       "9               0      1          5\n",
       "                1      1        104\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_analysis.groupby(['logit_1_decile', 'label'])['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logit_0_decile  label  class\n",
       "0               0      1          3\n",
       "                1      1        106\n",
       "1               0      1          9\n",
       "                1      1         99\n",
       "2               0      1         13\n",
       "                1      1         96\n",
       "3               0      1         12\n",
       "                1      1         96\n",
       "4               0      1         25\n",
       "                       0          1\n",
       "                1      1         81\n",
       "                       0          2\n",
       "5               0      0         26\n",
       "                       1         25\n",
       "                1      1         39\n",
       "                       0         18\n",
       "6               0      0         71\n",
       "                1      0         35\n",
       "                       1          2\n",
       "7               0      0         77\n",
       "                1      0         32\n",
       "8               0      0         92\n",
       "                1      0         16\n",
       "9               0      0        102\n",
       "                1      0          7\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_analysis.groupby(['logit_0_decile', 'label'])['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label  class\n",
       "0      0        369\n",
       "       1         87\n",
       "1      1        519\n",
       "       0        110\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_analysis.groupby('label')['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in testing_analysis.loc[testing_analysis.logit_1_decile == 9].index[0:20]:\n",
    "    plt.figure()\n",
    "    row = testing_analysis.iloc[i]\n",
    "    plt.title('class %f label %f'%(row['class'], row['label']))\n",
    "    imshow(Image.fromarray((X_test[i,:].reshape((200,200))).astype('uint8'), mode = 'L'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUVs and Verts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv('/home/ec2-user/training_data_meta.csv')\n",
    "\n",
    "#Join cleaned class data with training metadata\n",
    "\n",
    "class_data = pd.read_csv('/home/ec2-user/stanford_labels_cleaned.csv')\n",
    "\n",
    "meta_data = pd.merge(meta_data, class_data, on = 'class', how = 'left')\n",
    "\n",
    "meta_data = meta_data.loc[meta_data['Body Type'].isin(['SUV', 'Convertible'\n",
    "                                                      ])].copy()\n",
    "\n",
    "meta_data['is_sedan_target'] = (meta_data['Body Type'] == 'SUV').astype(int)\n",
    "\n",
    "image_dict = p.load(open('/home/ec2-user/scaled_bounded_grayscale_dict.p', 'rb'))\n",
    "\n",
    "oot_data = []\n",
    "for i in meta_data[['is_sedan_target', 'fname']].iterrows():\n",
    "    row = [i[1]['is_sedan_target']]\n",
    "    row.extend(image_dict[i[1]['fname']].flatten())\n",
    "    oot_data.append(row)\n",
    "\n",
    "oot_data = np.array(oot_data).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oot = oot_data[:,1:]\n",
    "\n",
    "y_oot = oot_data[:,0].astype('int32')\n",
    "y_oot = np.concatenate([1 - y_oot.reshape(-1,1), y_oot.reshape(-1,1)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oot error\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-21T15:51:47Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/convnet_model/model.ckpt-5653\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-21-15:52:17\n",
      "INFO:tensorflow:Saving dict for global step 5653: eval_accuracy = 0.82551754, global_step = 5653, loss = 0.80654323\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5653: /home/ec2-user/convnet_model/model.ckpt-5653\n",
      "{'eval_accuracy': 0.82551754, 'loss': 0.80654323, 'global_step': 5653}\n"
     ]
    }
   ],
   "source": [
    "print('oot error')\n",
    "eval_results = car_classifier.evaluate(\n",
    "    input_fn=lambda:image_processor_eval_input_fn(X_oot, y_oot, 32))\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/convnet_model/model.ckpt-5653\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predicted_classes = car_classifier.predict(\n",
    "    input_fn=lambda:image_processor_eval_input_fn(X_oot, y_oot, 32))\n",
    "\n",
    "testing_analysis = []\n",
    "\n",
    "for pred, label in zip(predicted_classes, y_oot):\n",
    "    hold = [pred['probabilities'][1], pred['classes'], label[1], pred['logits'][0], pred['logits'][1]]\n",
    "    testing_analysis.append(hold)\n",
    "    \n",
    "testing_analysis = pd.DataFrame(testing_analysis, columns = ['prob', 'class', 'label', 'logit_0', 'logit_1'])\n",
    "testing_analysis['logit_1_decile'] = pd.qcut(testing_analysis.logit_1, 10, labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logit_1_decile  label  class\n",
       "0               0      0        231\n",
       "                1      0         29\n",
       "1               0      0        196\n",
       "                1      0         63\n",
       "2               0      0        160\n",
       "                1      0         97\n",
       "                       1          2\n",
       "3               0      1         82\n",
       "                       0         40\n",
       "                1      1        104\n",
       "                       0         34\n",
       "4               0      1        113\n",
       "                1      1        146\n",
       "5               0      1         68\n",
       "                1      1        191\n",
       "6               0      1         49\n",
       "                1      1        211\n",
       "7               0      1         40\n",
       "                1      1        219\n",
       "8               0      1         30\n",
       "                1      1        229\n",
       "9               0      1         27\n",
       "                1      1        233\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_analysis.groupby(['logit_1_decile', 'label'])['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label  class\n",
       "0      0         627\n",
       "       1         409\n",
       "1      1        1335\n",
       "       0         223\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_analysis.groupby('label')['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in testing_analysis.loc[testing_analysis.logit_1_decile == 3].index[0:20]:\n",
    "    plt.figure()\n",
    "    row = testing_analysis.iloc[i]\n",
    "    plt.title('class %f label %f'%(row['class'], row['label']))\n",
    "    imshow(Image.fromarray((X_oot[i,:].reshape((200,200))).astype('uint8'), mode = 'L'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(\n",
    "     input_fn=lambda:image_processor_train_input_fn(\n",
    "         training_image_processor, X_train, y_train, 32), \n",
    "     max_steps=4100)\n",
    "\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=lambda:image_processor_eval_input_fn(X_train, y_train, 32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "INFO:tensorflow:Skipping training since max_steps has already saved.\n"
     ]
    }
   ],
   "source": [
    "evaluate_results = tf.estimator.train_and_evaluate(car_classifier, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    inputs = {\n",
    "        'input' : tf.placeholder(tf.float32, [None, 200, 200, 1]),\n",
    "    }\n",
    "    \n",
    "    inputs['input'] = tf.divide(inputs['input'], 255.0)\n",
    "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/convnet_model/model.ckpt-4100\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/ec2-user/convnet_saved_model/temp-b'1553120111'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'/home/ec2-user/convnet_saved_model/1553120111'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_classifier.export_savedmodel('/home/ec2-user/convnet_saved_model/', serving_input_receiver_fn,\n",
    "                            strip_default_attrs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
