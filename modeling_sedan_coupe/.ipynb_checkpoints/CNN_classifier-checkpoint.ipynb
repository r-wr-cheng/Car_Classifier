{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as p\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#tf.enable_eager_execution()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv('/home/ec2-user/training_data_meta.csv')\n",
    "\n",
    "#Join cleaned class data with training metadata\n",
    "\n",
    "class_data = pd.read_csv('/home/ec2-user/stanford_labels_cleaned.csv')\n",
    "\n",
    "meta_data = pd.merge(meta_data, class_data, on = 'class', how = 'left')\n",
    "\n",
    "meta_data = meta_data.loc[meta_data['Body Type'].isin(['Coupe', 'Sedan'])].copy()\n",
    "\n",
    "meta_data['is_sedan_target'] = (meta_data['Body Type'] == 'Sedan').astype(int)\n",
    "\n",
    "image_dict = p.load(open('/home/ec2-user/scaled_bounded_grayscale_dict.p', 'rb'))\n",
    "\n",
    "training_data = []\n",
    "for i in meta_data[['is_sedan_target', 'fname']].iterrows():\n",
    "    row = [i[1]['is_sedan_target']]\n",
    "    row.extend(image_dict[i[1]['fname']].flatten())\n",
    "    training_data.append(row)\n",
    "\n",
    "training_data = np.array(training_data).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training_data[:,1:], \n",
    "                                                    training_data[:,0], \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=420)\n",
    "\n",
    "y_train = y_train.astype('int32')\n",
    "y_test = y_test.astype('int32')\n",
    "\n",
    "negatives = []\n",
    "for ind, elem in enumerate(y_train):\n",
    "    if elem == 0:\n",
    "        negatives.append(ind)\n",
    "np.random.shuffle(negatives)\n",
    "\n",
    "neg_dupe_target = 2 * y_train.sum() - y_train.shape[0]\n",
    "\n",
    "X_train = np.concatenate([X_train, X_train[negatives[0:neg_dupe_target],:]],\n",
    "                         axis = 0\n",
    "                        )\n",
    "y_train = np.concatenate([y_train, y_train[negatives[0:neg_dupe_target]]],\n",
    "                         axis = 0\n",
    "                        )\n",
    "\n",
    "y_train = np.concatenate([1 - y_train.reshape(-1,1), y_train.reshape(-1,1)], axis = 1)\n",
    "y_test = np.concatenate([1 - y_test.reshape(-1,1), y_test.reshape(-1,1)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    if type(features) is dict:\n",
    "        features = features['input']\n",
    "    #Define model architecture\n",
    "    \n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    input_layer = tf.reshape(features, [-1, 200, 200, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 3x3 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 200, 200, 1]\n",
    "    # Output Tensor Shape: [batch_size, 200, 200, 32]\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 200, 200, 32]\n",
    "    # Output Tensor Shape: [batch_size, 100, 100, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 3x3 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 100, 100, 32]\n",
    "    # Output Tensor Shape: [batch_size, 100, 100, 64]\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 100, 100, 64]\n",
    "    # Output Tensor Shape: [batch_size, 50, 50, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(\n",
    "      inputs=pool2,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #3\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 50, 50, 64]\n",
    "    # Output Tensor Shape: [batch_size, 25, 25, 64]\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 25, 25, 64]\n",
    "    # Output Tensor Shape: [batch_size, 25 * 25 * 64]\n",
    "    pool3_flat = tf.reshape(pool3, [-1, 25 * 25 * 64])\n",
    "\n",
    "    # Dense Layer\n",
    "    # Densely connected layer with 256 neurons\n",
    "    # Input Tensor Shape: [batch_size, 25 * 25 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 256]\n",
    "    dense = tf.layers.dense(inputs=pool3_flat, units=256, activation=tf.nn.relu)\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.5, training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=2)\n",
    "    \n",
    "    #Predict Op\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            # Generate predictions (for PREDICT and EVAL mode)\n",
    "            \"classes\": tf.argmax(input=logits, axis=1),\n",
    "            # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "            # `logging_hook`.\n",
    "            \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\"),\n",
    "            \"logits\" : logits\n",
    "            \n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        #Define model outputs\n",
    "        # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels[:,1], logits=logits)\n",
    "        \n",
    "        logging_hook = tf.train.LoggingTensorHook(\n",
    "            {\"loss\" : loss,\n",
    "            },\n",
    "            every_n_iter=10)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, loss=loss, train_op=train_op, training_hooks = [logging_hook])\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        #Define model outputs\n",
    "        # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels[:,1], logits=logits)\n",
    "        \n",
    "        predictions = {\n",
    "          # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "          # `logging_hook`.\n",
    "          \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "        }\n",
    "        #swap this for AUROC?\n",
    "        eval_metric_ops = {\n",
    "          \"eval_accuracy\": tf.metrics.auc(\n",
    "              labels=labels, predictions=predictions[\"probabilities\"])}\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "          mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/home/ec2-user/convnet_model', '_tf_random_seed': None, '_save_summary_steps': 10, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': intra_op_parallelism_threads: 32\n",
      "inter_op_parallelism_threads: 32\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f75357b5d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "session_config = tf.ConfigProto(\n",
    "      inter_op_parallelism_threads=32,\n",
    "      intra_op_parallelism_threads=32,\n",
    "      )\n",
    "\n",
    "config = tf.estimator.RunConfig(session_config = session_config,\n",
    "                                log_step_count_steps = 10,\n",
    "                                save_summary_steps = 10\n",
    "                               )\n",
    "\n",
    "# Create the Estimator\n",
    "car_classifier = tf.estimator.Estimator(\n",
    "  model_fn=cnn_model_fn, model_dir=\"/home/ec2-user/convnet_model\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#custom input function using training_image_processor with tf.data.Dataset\n",
    "def image_processor_train_input_fn(image_processor, X_train, y_train, batch_size):\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator = lambda : image_processor.flow(X_train.reshape((X_train.shape[0], 200, 200, 1)), \n",
    "                                                  y_train, batch_size, shuffle = True), \n",
    "        output_types = (np.float32, np.int32))\n",
    "    \n",
    "    dataset = dataset.shuffle(512).repeat()\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def image_processor_eval_input_fn(X_test, y_test, batch_size):\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_test.astype('float32') / 255.0, \n",
    "                                                  y_test.astype('int32')))\n",
    "        \n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "#Shearing, no zooming\n",
    "training_image_processor = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale= 1./255,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    shear_range=15,\n",
    "    horizontal_flip=True,\n",
    "    data_format = 'channels_last',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/convnet_model/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /home/ec2-user/convnet_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.3248356, step = 2001\n",
      "INFO:tensorflow:loss = 0.3248356\n",
      "INFO:tensorflow:global_step/sec: 0.253579\n",
      "INFO:tensorflow:loss = 0.3441033, step = 2011 (39.437 sec)\n",
      "INFO:tensorflow:loss = 0.3441033 (39.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.302291\n",
      "INFO:tensorflow:loss = 0.2703493, step = 2021 (33.080 sec)\n",
      "INFO:tensorflow:loss = 0.2703493 (33.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.305158\n",
      "INFO:tensorflow:loss = 0.41520917, step = 2031 (32.770 sec)\n",
      "INFO:tensorflow:loss = 0.41520917 (32.770 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.307353\n",
      "INFO:tensorflow:loss = 0.21544191, step = 2041 (32.536 sec)\n",
      "INFO:tensorflow:loss = 0.21544191 (32.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.294563\n",
      "INFO:tensorflow:loss = 0.33473867, step = 2051 (33.948 sec)\n",
      "INFO:tensorflow:loss = 0.33473867 (33.948 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.293682\n",
      "INFO:tensorflow:loss = 0.24005482, step = 2061 (34.051 sec)\n",
      "INFO:tensorflow:loss = 0.24005482 (34.050 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.293678\n",
      "INFO:tensorflow:loss = 0.35831976, step = 2071 (34.051 sec)\n",
      "INFO:tensorflow:loss = 0.35831976 (34.051 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.306003\n",
      "INFO:tensorflow:loss = 0.2576354, step = 2081 (32.679 sec)\n",
      "INFO:tensorflow:loss = 0.2576354 (32.679 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.296576\n",
      "INFO:tensorflow:loss = 0.2389964, step = 2091 (33.718 sec)\n",
      "INFO:tensorflow:loss = 0.2389964 (33.718 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.294346\n",
      "INFO:tensorflow:loss = 0.2477074, step = 2101 (33.974 sec)\n",
      "INFO:tensorflow:loss = 0.2477074 (33.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.296427\n",
      "INFO:tensorflow:loss = 0.27468842, step = 2111 (33.735 sec)\n",
      "INFO:tensorflow:loss = 0.27468842 (33.735 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.295007\n",
      "INFO:tensorflow:loss = 0.27437282, step = 2121 (33.898 sec)\n",
      "INFO:tensorflow:loss = 0.27437282 (33.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.293847\n",
      "INFO:tensorflow:loss = 0.26806203, step = 2131 (34.031 sec)\n",
      "INFO:tensorflow:loss = 0.26806203 (34.031 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2132 into /home/ec2-user/convnet_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.308027\n",
      "INFO:tensorflow:loss = 0.26127568, step = 2141 (32.465 sec)\n",
      "INFO:tensorflow:loss = 0.26127568 (32.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.294466\n",
      "INFO:tensorflow:loss = 0.2576074, step = 2151 (33.960 sec)\n",
      "INFO:tensorflow:loss = 0.2576074 (33.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.308349\n",
      "INFO:tensorflow:loss = 0.26169705, step = 2161 (32.431 sec)\n",
      "INFO:tensorflow:loss = 0.26169705 (32.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.293253\n",
      "INFO:tensorflow:loss = 0.2652046, step = 2171 (34.100 sec)\n",
      "INFO:tensorflow:loss = 0.2652046 (34.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.295316\n",
      "INFO:tensorflow:loss = 0.2162199, step = 2181 (33.862 sec)\n",
      "INFO:tensorflow:loss = 0.2162199 (33.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.307227\n",
      "INFO:tensorflow:loss = 0.26718462, step = 2191 (32.549 sec)\n",
      "INFO:tensorflow:loss = 0.26718462 (32.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.306381\n",
      "INFO:tensorflow:loss = 0.2177133, step = 2201 (32.639 sec)\n",
      "INFO:tensorflow:loss = 0.2177133 (32.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.294287\n",
      "INFO:tensorflow:loss = 0.28518435, step = 2211 (33.980 sec)\n",
      "INFO:tensorflow:loss = 0.28518435 (33.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.297519\n",
      "INFO:tensorflow:loss = 0.24028282, step = 2221 (33.611 sec)\n",
      "INFO:tensorflow:loss = 0.24028282 (33.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.299322\n",
      "INFO:tensorflow:loss = 0.32084364, step = 2231 (33.409 sec)\n",
      "INFO:tensorflow:loss = 0.32084364 (33.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.295507\n",
      "INFO:tensorflow:loss = 0.25198135, step = 2241 (33.840 sec)\n",
      "INFO:tensorflow:loss = 0.25198135 (33.840 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.296246\n",
      "INFO:tensorflow:loss = 0.27786398, step = 2251 (33.756 sec)\n",
      "INFO:tensorflow:loss = 0.27786398 (33.756 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.304561\n",
      "INFO:tensorflow:loss = 0.2755053, step = 2261 (32.834 sec)\n",
      "INFO:tensorflow:loss = 0.2755053 (32.834 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.293613\n",
      "INFO:tensorflow:loss = 0.23705488, step = 2271 (34.058 sec)\n",
      "INFO:tensorflow:loss = 0.23705488 (34.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.295971\n",
      "INFO:tensorflow:loss = 0.31893545, step = 2281 (33.787 sec)\n",
      "INFO:tensorflow:loss = 0.31893545 (33.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.309242\n",
      "INFO:tensorflow:loss = 0.31451097, step = 2291 (32.337 sec)\n",
      "INFO:tensorflow:loss = 0.31451097 (32.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.296955\n",
      "INFO:tensorflow:loss = 0.19440028, step = 2301 (33.675 sec)\n",
      "INFO:tensorflow:loss = 0.19440028 (33.675 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.294913\n",
      "INFO:tensorflow:loss = 0.27756914, step = 2311 (33.908 sec)\n",
      "INFO:tensorflow:loss = 0.27756914 (33.908 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2312 into /home/ec2-user/convnet_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.331306\n",
      "INFO:tensorflow:loss = 0.20722322, step = 2321 (30.184 sec)\n",
      "INFO:tensorflow:loss = 0.20722322 (30.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.298777\n",
      "INFO:tensorflow:loss = 0.17003196, step = 2331 (33.470 sec)\n",
      "INFO:tensorflow:loss = 0.17003196 (33.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.299857\n",
      "INFO:tensorflow:loss = 0.1999446, step = 2341 (33.349 sec)\n",
      "INFO:tensorflow:loss = 0.1999446 (33.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.295648\n",
      "INFO:tensorflow:loss = 0.36268908, step = 2351 (33.824 sec)\n",
      "INFO:tensorflow:loss = 0.36268908 (33.824 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.307004\n",
      "INFO:tensorflow:loss = 0.30641028, step = 2361 (32.573 sec)\n",
      "INFO:tensorflow:loss = 0.30641028 (32.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.295821\n",
      "INFO:tensorflow:loss = 0.19043761, step = 2371 (33.804 sec)\n",
      "INFO:tensorflow:loss = 0.19043761 (33.804 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.296372\n",
      "INFO:tensorflow:loss = 0.25069577, step = 2381 (33.741 sec)\n",
      "INFO:tensorflow:loss = 0.25069577 (33.741 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.297823\n",
      "INFO:tensorflow:loss = 0.26387772, step = 2391 (33.577 sec)\n",
      "INFO:tensorflow:loss = 0.26387772 (33.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.307332\n",
      "INFO:tensorflow:loss = 0.2614727, step = 2401 (32.538 sec)\n",
      "INFO:tensorflow:loss = 0.2614727 (32.538 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.307956\n",
      "INFO:tensorflow:loss = 0.2794514, step = 2411 (32.472 sec)\n",
      "INFO:tensorflow:loss = 0.2794514 (32.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.30712\n",
      "INFO:tensorflow:loss = 0.264459, step = 2421 (32.560 sec)\n",
      "INFO:tensorflow:loss = 0.264459 (32.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.329309\n",
      "INFO:tensorflow:loss = 0.25832742, step = 2431 (30.367 sec)\n",
      "INFO:tensorflow:loss = 0.25832742 (30.366 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.295939\n",
      "INFO:tensorflow:loss = 0.30262348, step = 2441 (33.791 sec)\n",
      "INFO:tensorflow:loss = 0.30262348 (33.791 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.293842\n",
      "INFO:tensorflow:loss = 0.22739533, step = 2451 (34.032 sec)\n",
      "INFO:tensorflow:loss = 0.22739533 (34.032 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.304404\n",
      "INFO:tensorflow:loss = 0.24071549, step = 2461 (32.851 sec)\n",
      "INFO:tensorflow:loss = 0.24071549 (32.851 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.292822\n",
      "INFO:tensorflow:loss = 0.25363553, step = 2471 (34.150 sec)\n",
      "INFO:tensorflow:loss = 0.25363553 (34.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.294798\n",
      "INFO:tensorflow:loss = 0.2587291, step = 2481 (33.922 sec)\n",
      "INFO:tensorflow:loss = 0.2587291 (33.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.304256\n",
      "INFO:tensorflow:loss = 0.21476878, step = 2491 (32.867 sec)\n",
      "INFO:tensorflow:loss = 0.21476878 (32.867 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2494 into /home/ec2-user/convnet_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.29393\n",
      "INFO:tensorflow:loss = 0.25971445, step = 2501 (34.022 sec)\n",
      "INFO:tensorflow:loss = 0.25971445 (34.022 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.295068\n",
      "INFO:tensorflow:loss = 0.28851736, step = 2511 (33.891 sec)\n",
      "INFO:tensorflow:loss = 0.28851736 (33.891 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.296071\n",
      "INFO:tensorflow:loss = 0.25402114, step = 2521 (33.776 sec)\n",
      "INFO:tensorflow:loss = 0.25402114 (33.776 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.318947\n",
      "INFO:tensorflow:loss = 0.176983, step = 2531 (31.353 sec)\n",
      "INFO:tensorflow:loss = 0.176983 (31.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.303927\n",
      "INFO:tensorflow:loss = 0.28167146, step = 2541 (32.903 sec)\n",
      "INFO:tensorflow:loss = 0.28167146 (32.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.298263\n",
      "INFO:tensorflow:loss = 0.20035866, step = 2551 (33.528 sec)\n",
      "INFO:tensorflow:loss = 0.20035866 (33.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.295796\n",
      "INFO:tensorflow:loss = 0.16533354, step = 2561 (33.807 sec)\n",
      "INFO:tensorflow:loss = 0.16533354 (33.807 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.297268\n",
      "INFO:tensorflow:loss = 0.20754793, step = 2571 (33.640 sec)\n",
      "INFO:tensorflow:loss = 0.20754793 (33.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.295934\n",
      "INFO:tensorflow:loss = 0.1752066, step = 2581 (33.791 sec)\n",
      "INFO:tensorflow:loss = 0.1752066 (33.791 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.306086\n",
      "INFO:tensorflow:loss = 0.25453645, step = 2591 (32.671 sec)\n",
      "INFO:tensorflow:loss = 0.25453645 (32.671 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.295161\n",
      "INFO:tensorflow:loss = 0.2956593, step = 2601 (33.880 sec)\n",
      "INFO:tensorflow:loss = 0.2956593 (33.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.307729\n",
      "INFO:tensorflow:loss = 0.21121503, step = 2611 (32.496 sec)\n",
      "INFO:tensorflow:loss = 0.21121503 (32.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.297424\n",
      "INFO:tensorflow:loss = 0.18043324, step = 2621 (33.622 sec)\n",
      "INFO:tensorflow:loss = 0.18043324 (33.622 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.295379\n",
      "INFO:tensorflow:loss = 0.24768305, step = 2631 (33.855 sec)\n",
      "INFO:tensorflow:loss = 0.24768305 (33.855 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.298359\n",
      "INFO:tensorflow:loss = 0.20615497, step = 2641 (33.517 sec)\n",
      "INFO:tensorflow:loss = 0.20615497 (33.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.29537\n",
      "INFO:tensorflow:loss = 0.23646915, step = 2651 (33.856 sec)\n",
      "INFO:tensorflow:loss = 0.23646915 (33.856 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.294097\n",
      "INFO:tensorflow:loss = 0.2214377, step = 2661 (34.002 sec)\n",
      "INFO:tensorflow:loss = 0.2214377 (34.002 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.307884\n",
      "INFO:tensorflow:loss = 0.27129066, step = 2671 (32.480 sec)\n",
      "INFO:tensorflow:loss = 0.27129066 (32.480 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2674 into /home/ec2-user/convnet_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.303454\n",
      "INFO:tensorflow:loss = 0.15433624, step = 2681 (32.954 sec)\n",
      "INFO:tensorflow:loss = 0.15433624 (32.954 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.29633\n",
      "INFO:tensorflow:loss = 0.17627251, step = 2691 (33.746 sec)\n",
      "INFO:tensorflow:loss = 0.17627251 (33.746 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.295438\n",
      "INFO:tensorflow:loss = 0.3133675, step = 2701 (33.848 sec)\n",
      "INFO:tensorflow:loss = 0.3133675 (33.848 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.305528\n",
      "INFO:tensorflow:loss = 0.25603896, step = 2711 (32.730 sec)\n",
      "INFO:tensorflow:loss = 0.25603896 (32.730 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.305562\n",
      "INFO:tensorflow:loss = 0.19183807, step = 2721 (32.727 sec)\n",
      "INFO:tensorflow:loss = 0.19183807 (32.727 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.308482\n",
      "INFO:tensorflow:loss = 0.23637621, step = 2731 (32.417 sec)\n",
      "INFO:tensorflow:loss = 0.23637621 (32.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.309725\n",
      "INFO:tensorflow:loss = 0.13718778, step = 2741 (32.287 sec)\n",
      "INFO:tensorflow:loss = 0.13718778 (32.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.309216\n",
      "INFO:tensorflow:loss = 0.1622681, step = 2751 (32.340 sec)\n",
      "INFO:tensorflow:loss = 0.1622681 (32.340 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-88df287ddb9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m car_classifier.train(\n\u001b[1;32m      2\u001b[0m       \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimage_processor_train_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_image_processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m       \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1157\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1405\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m         \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    674\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1169\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1172\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "car_classifier.train(\n",
    "      input_fn=lambda:image_processor_train_input_fn(training_image_processor, X_train, y_train, 128),\n",
    "      steps=1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From <ipython-input-33-c53fe323e3d9>:21: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-33-c53fe323e3d9>:27: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-33-c53fe323e3d9>:69: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-33-c53fe323e3d9>:73: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:788: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-06T21:14:31Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/convnet_model/model.ckpt-10003\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-06-21:15:43\n",
      "INFO:tensorflow:Saving dict for global step 10003: eval_accuracy = 0.9980397, global_step = 10003, loss = 0.049818352\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10003: /home/ec2-user/convnet_model/model.ckpt-10003\n",
      "{'eval_accuracy': 0.9980397, 'loss': 0.049818352, 'global_step': 10003}\n",
      "testing error\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-06T21:15:49Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/convnet_model/model.ckpt-10003\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-06-21:16:13\n",
      "INFO:tensorflow:Saving dict for global step 10003: eval_accuracy = 0.86896265, global_step = 10003, loss = 0.7949253\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10003: /home/ec2-user/convnet_model/model.ckpt-10003\n",
      "{'eval_accuracy': 0.86896265, 'loss': 0.7949253, 'global_step': 10003}\n"
     ]
    }
   ],
   "source": [
    "print('training error')\n",
    "eval_results = car_classifier.evaluate(\n",
    "    input_fn=lambda:image_processor_eval_input_fn(X_train, y_train, 32))\n",
    "print(eval_results)\n",
    "\n",
    "print('testing error')\n",
    "eval_results = car_classifier.evaluate(\n",
    "    input_fn=lambda:image_processor_eval_input_fn(X_test, y_test, 32))\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/convnet_model/model.ckpt-10003\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predicted_classes = car_classifier.predict(\n",
    "    input_fn=lambda:image_processor_eval_input_fn(X_test, y_test, 32))\n",
    "\n",
    "testing_analysis = []\n",
    "\n",
    "for pred, label in zip(predicted_classes, y_test):\n",
    "    hold = [pred['probabilities'][1], pred['classes'], label[1], pred['logits'][0], pred['logits'][1]]\n",
    "    testing_analysis.append(hold)\n",
    "    \n",
    "testing_analysis = pd.DataFrame(testing_analysis, columns = ['prob', 'class', 'label', 'logit_0', 'logit_1'])\n",
    "testing_analysis['logit_1_decile'] = pd.qcut(testing_analysis.logit_1, 10, labels = False)\n",
    "testing_analysis['logit_0_decile'] = pd.qcut(testing_analysis.logit_0, 10, labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logit_1_decile  label  class\n",
       "0               0      0        101\n",
       "                1      0          8\n",
       "1               0      0         92\n",
       "                1      0         16\n",
       "2               0      0         82\n",
       "                1      0         27\n",
       "3               0      0         58\n",
       "                1      0         47\n",
       "                       1          3\n",
       "4               0      0         37\n",
       "                       1         13\n",
       "                1      0         40\n",
       "                       1         19\n",
       "5               0      1         22\n",
       "                       0          4\n",
       "                1      1         75\n",
       "                       0          7\n",
       "6               0      1         29\n",
       "                1      1         79\n",
       "7               0      1         10\n",
       "                1      1         99\n",
       "8               0      1          5\n",
       "                1      1        103\n",
       "9               0      1          3\n",
       "                1      1        106\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_analysis.groupby(['logit_1_decile', 'label'])['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logit_0_decile  label  class\n",
       "0               1      1        109\n",
       "1               0      1          9\n",
       "                1      1         99\n",
       "2               0      1         11\n",
       "                1      1         98\n",
       "3               0      1         18\n",
       "                1      1         90\n",
       "4               0      1         30\n",
       "                       0          4\n",
       "                1      1         73\n",
       "                       0          2\n",
       "5               0      0         40\n",
       "                       1         13\n",
       "                1      0         40\n",
       "                       1         15\n",
       "6               0      0         59\n",
       "                       1          1\n",
       "                1      0         48\n",
       "7               0      0         77\n",
       "                1      0         32\n",
       "8               0      0         92\n",
       "                1      0         16\n",
       "9               0      0        102\n",
       "                1      0          7\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_analysis.groupby(['logit_0_decile', 'label'])['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label  class\n",
       "0      0        374\n",
       "       1         82\n",
       "1      1        484\n",
       "       0        145\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_analysis.groupby('label')['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in testing_analysis.loc[testing_analysis.logit_1_decile == 9].index[0:20]:\n",
    "    plt.figure()\n",
    "    row = testing_analysis.iloc[i]\n",
    "    plt.title('class %f label %f'%(row['class'], row['label']))\n",
    "    imshow(Image.fromarray((X_test[i,:].reshape((200,200))).astype('uint8'), mode = 'L'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUVs and Verts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv('/home/ec2-user/training_data_meta.csv')\n",
    "\n",
    "#Join cleaned class data with training metadata\n",
    "\n",
    "class_data = pd.read_csv('/home/ec2-user/stanford_labels_cleaned.csv')\n",
    "\n",
    "meta_data = pd.merge(meta_data, class_data, on = 'class', how = 'left')\n",
    "\n",
    "meta_data = meta_data.loc[meta_data['Body Type'].isin(['SUV', 'Convertible'\n",
    "                                                      ])].copy()\n",
    "\n",
    "meta_data['is_sedan_target'] = (meta_data['Body Type'] == 'SUV').astype(int)\n",
    "\n",
    "image_dict = p.load(open('/home/ec2-user/scaled_bounded_grayscale_dict.p', 'rb'))\n",
    "\n",
    "oot_data = []\n",
    "for i in meta_data[['is_sedan_target', 'fname']].iterrows():\n",
    "    row = [i[1]['is_sedan_target']]\n",
    "    row.extend(image_dict[i[1]['fname']].flatten())\n",
    "    oot_data.append(row)\n",
    "\n",
    "oot_data = np.array(oot_data).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_oot = oot_data[:,1:]\n",
    "\n",
    "y_oot = oot_data[:,0].astype('int32')\n",
    "y_oot = np.concatenate([1 - y_oot.reshape(-1,1), y_oot.reshape(-1,1)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oot error\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-21T15:51:47Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/convnet_model/model.ckpt-5653\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-21-15:52:17\n",
      "INFO:tensorflow:Saving dict for global step 5653: eval_accuracy = 0.82551754, global_step = 5653, loss = 0.80654323\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5653: /home/ec2-user/convnet_model/model.ckpt-5653\n",
      "{'eval_accuracy': 0.82551754, 'loss': 0.80654323, 'global_step': 5653}\n"
     ]
    }
   ],
   "source": [
    "print('oot error')\n",
    "eval_results = car_classifier.evaluate(\n",
    "    input_fn=lambda:image_processor_eval_input_fn(X_oot, y_oot, 32))\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/convnet_model/model.ckpt-5653\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predicted_classes = car_classifier.predict(\n",
    "    input_fn=lambda:image_processor_eval_input_fn(X_oot, y_oot, 32))\n",
    "\n",
    "testing_analysis = []\n",
    "\n",
    "for pred, label in zip(predicted_classes, y_oot):\n",
    "    hold = [pred['probabilities'][1], pred['classes'], label[1], pred['logits'][0], pred['logits'][1]]\n",
    "    testing_analysis.append(hold)\n",
    "    \n",
    "testing_analysis = pd.DataFrame(testing_analysis, columns = ['prob', 'class', 'label', 'logit_0', 'logit_1'])\n",
    "testing_analysis['logit_1_decile'] = pd.qcut(testing_analysis.logit_1, 10, labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logit_1_decile  label  class\n",
       "0               0      0        231\n",
       "                1      0         29\n",
       "1               0      0        196\n",
       "                1      0         63\n",
       "2               0      0        160\n",
       "                1      0         97\n",
       "                       1          2\n",
       "3               0      1         82\n",
       "                       0         40\n",
       "                1      1        104\n",
       "                       0         34\n",
       "4               0      1        113\n",
       "                1      1        146\n",
       "5               0      1         68\n",
       "                1      1        191\n",
       "6               0      1         49\n",
       "                1      1        211\n",
       "7               0      1         40\n",
       "                1      1        219\n",
       "8               0      1         30\n",
       "                1      1        229\n",
       "9               0      1         27\n",
       "                1      1        233\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_analysis.groupby(['logit_1_decile', 'label'])['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label  class\n",
       "0      0         627\n",
       "       1         409\n",
       "1      1        1335\n",
       "       0         223\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_analysis.groupby('label')['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in testing_analysis.loc[testing_analysis.logit_1_decile == 3].index[0:20]:\n",
    "    plt.figure()\n",
    "    row = testing_analysis.iloc[i]\n",
    "    plt.title('class %f label %f'%(row['class'], row['label']))\n",
    "    imshow(Image.fromarray((X_oot[i,:].reshape((200,200))).astype('uint8'), mode = 'L'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(\n",
    "     input_fn=lambda:image_processor_train_input_fn(\n",
    "         training_image_processor, X_train, y_train, 32), \n",
    "     max_steps=4100)\n",
    "\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=lambda:image_processor_eval_input_fn(X_train, y_train, 32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "INFO:tensorflow:Skipping training since max_steps has already saved.\n"
     ]
    }
   ],
   "source": [
    "evaluate_results = tf.estimator.train_and_evaluate(car_classifier, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    inputs = {\n",
    "        'input' : tf.placeholder(tf.float32, [None, 200, 200, 1]),\n",
    "    }\n",
    "    \n",
    "    inputs['input'] = tf.divide(inputs['input'], 255.0)\n",
    "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/convnet_model/model.ckpt-4100\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/ec2-user/convnet_saved_model/temp-b'1553120111'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'/home/ec2-user/convnet_saved_model/1553120111'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_classifier.export_savedmodel('/home/ec2-user/convnet_saved_model/', serving_input_receiver_fn,\n",
    "                            strip_default_attrs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke SGM Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('runtime.sagemaker', region_name = 'us-east-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in X_train:\n",
    "    row = ''\n",
    "    for j in i:\n",
    "        scaled = j / 255\n",
    "        row += f'{scaled},'\n",
    "    data.append(row[:-1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = client.invoke_endpoint(EndpointName='sagemaker-tensorflow-scriptmode-2019-04-07-18-20-33-352',\n",
    "                                       ContentType='text/csv',\n",
    "                                       Body=data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"predictions\": [\n",
      "        {\n",
      "            \"probabilities\": [0.210038, 0.789962],\n",
      "            \"logits\": [-0.927644, 0.397055],\n",
      "            \"classes\": 1\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "hold = response['Body'].read().decode()\n",
    "\n",
    "print(hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/convnet_model/model.ckpt-10003\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "{'classes': 1, 'probabilities': array([0.01318983, 0.9868102 ], dtype=float32), 'logits': array([-2.8551736,  1.4598585], dtype=float32)}\n",
      "[0 1]\n",
      "{'classes': 1, 'probabilities': array([1.9244720e-04, 9.9980754e-01], dtype=float32), 'logits': array([-5.697554,  2.857942], dtype=float32)}\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "local_pred = car_classifier.predict(\n",
    "    input_fn=lambda:image_processor_eval_input_fn(X_train[0:2], y_train[0:2], 32))\n",
    "\n",
    "for pred, label in zip(local_pred, y_train[0:2]):\n",
    "    print(pred)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].reshape((200,200)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=200x200 at 0x7F752C5A84A8>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(X_train[0].reshape((200,200)).astype('int8'), mode='L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[114, 113, 115, ..., 136, 101,  96],\n",
       "       [113, 113, 115, ..., 138, 101, 101],\n",
       "       [113, 113, 115, ..., 137,  94,  99],\n",
       "       ...,\n",
       "       [179, 174, 171, ...,  74, 158, 145],\n",
       "       [175, 173, 175, ...,  98,  85, 166],\n",
       "       [175, 175, 172, ...,  80, 106, 158]], dtype=uint8)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dict['00001.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
